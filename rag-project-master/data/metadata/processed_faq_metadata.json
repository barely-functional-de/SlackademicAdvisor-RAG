[{"question": "I just discovered the course. Can I still join?", "answer": "Yes, but if you want to receive a certificate, you need to submit your project while we\u2019re still accepting submissions.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "6c0ed807fa"}, {"question": "I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?", "answer": "You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "f15df47531"}, {"question": "What is the video/zoom link to the stream for the \u201cOffice Hours\u201d or live/workshop sessions?", "answer": "The zoom link is only published to instructors/presenters/TAs. Students participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel. Don\u2019t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "2b7307be44"}, {"question": "SaturnCloud - How do I get access?", "answer": "Issue: I get the notice that due to traffic, I\u2019m on a waitlist for new signups. Answer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for 'llm zoomcamp'. Issue: I\u2019m a pre-existing user from a different zoomcamp and I\u2019m not awarded the free hours even though I\u2019ve submitted my email in the form. Answer: Just request it via their chat, after you\u2019ve logged in using your pre-existing account, citing 'llm zoomcamp'.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "b3b1046d32"}, {"question": "SaturnCloud - How many free hours do we get?", "answer": "We get 15 free hours per month, which might be limited to the free tier\u2019s hardware configuration.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "c7b04a91ac"}, {"question": "SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month", "answer": "This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "2c9ce572bc"}, {"question": "Cloud alternatives with GPU", "answer": "Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted, or it might be billed separately. Google Colab, Kaggle, Databricks (?), so many others. Use GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "39fadd7c52"}, {"question": "Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?", "answer": "When you set up your account you are automatically assigned a random name such as 'Lucid Elbakyan' for example. Click on the Jump to your record on the leaderboard link to find your entry. If you want to see what your Display name is, click on the Edit Course Profile button. First field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous. Unless you want 'Lucid Elbakyan' on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver\u2019s license, etc. This is the name that is going to appear on your Certificate!", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "dd2ca2de5c"}, {"question": "Certificate - Can I follow the course in a self-paced mode and get a certificate?", "answer": "No, you can only get a certificate if you finish the course with a 'live' cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project. You can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "4781a2564a"}, {"question": "I missed the first homework - can I still get a certificate?", "answer": "Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "f16bea1f8d"}, {"question": "I was working on next week\u2019s homework/content - why does it keep changing?", "answer": "This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "57ac07b705"}, {"question": "When will the course be offered next?", "answer": "Summer 2025 (via Alexey).", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "c77202b829"}, {"question": "WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.", "answer": "Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\Users\\YourUsername\\.wslconfig) with the desired RAM allocation: [wsl2] memory=8GB. Restart WSL: wsl --shutdown. Run the free command to verify the changes. For more details, read this article.", "course": "llm-zoomcamp", "section": "General course-related questions", "id": "d1b8737ff0"}, {"question": "OpenAI: Error when running OpenAI chat.completions.create command", "answer": "You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account: NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "1cefc1a8bc"}, {"question": "OpenAI: Error: RateLimitError: Error code: 429", "answer": "RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'} The above errors are related to your OpenAI API account\u2019s quota. There is no free usage of OpenAI\u2019s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code. Steps to resolve: Add credits to your account here (min $5) In chat.completions.create(model='gpt-4o', \u2026) specify one of the available for you models: You might need to recreate an API key after adding credits to your account and update it locally.", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "54f0f0b59a"}, {"question": "OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?", "answer": "Update openai version from 0.27.0 -> any 1.x version", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "34561adadf"}, {"question": "OpenAI: How much will I have to spend to use the Open AI API?", "answer": "Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "2aae435f61"}, {"question": "OpenAI: Do I have to subscribe and pay for Open AI API for this course?", "answer": "No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github. llm-zoomcamp/01-intro/open-ai-alternatives.md at main \u00b7 DataTalksClub/llm-zoomcamp (github.com)", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "26a78d9143"}, {"question": "ElasticSearch: ERROR: Elasticsearch exited unexpectedly", "answer": "If you get this error, it\u2019s likely that elasticsearch doesn\u2019t get enough RAM I specified the RAM size to the configuration (-m 4GB) docker run -it --rm --name elasticsearch -m 4GB -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.4.3 Or give it _less_ RAM: Tip for Github Codespace users If you want to run elasticsearch server in a docker, then it may fail with the command in the documentation. In that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\". This reduces the resource usage. Full command: docker run -it --rm --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" docker.elastic.co/elasticsearch/elasticsearch:8.4.3 If it doesn't work, try this: sudo sysctl -w vm.max_map_count=262144 And give the Java machine inside the container more RAM: docker run -it --rm --name elasticsearch -p 9200:9200 -p 9300:9300 --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 --memory=4g --cpus=2 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" docker.elastic.co/elasticsearch/elasticsearch:8.4.3 Another possible solution may be to set the memory_lock to false: docker run -it --rm --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" -e \"bootstrap.memory_lock=false\" docker.elastic.co/elasticsearch/elasticsearch:8.4.3", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "19e7748b46"}, {"question": "ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'", "answer": "Instead of document as used in the course video, use doc", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "97de138417"}, {"question": "Docker: How do I store data persistently in Elasticsearch?", "answer": "When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping: docker volume create elasticsearch_data docker run -it --rm --name elasticsearch -p 9200:9200 -p 9300:9300 -v elasticsearch_data:/usr/share/elasticsearch/data -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.4.3", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "743854052d"}, {"question": "Authentication: Safe and easy way to store and load API keys", "answer": "You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file. For example, you can create a new file named \u201capi_keys.yml\u201d in your repository. Then, do not forget to add it in your .gitignore file: #api_keys api_keys.yml You can now fill your api_keys.yml file: OPENAI_API_KEY: \u201csk[...]\u201d GROQ_API_KEY: \u201cgqk_[...]\u201d Save your file. You will need the pyyaml library to load your yaml file, so run this command in your terminal: pip install pyyaml Now, open your jupyter notebook. You can load your yaml file and the associated keys with this code: import yaml # Open the file with open('api_keys.yml', 'r') as file: # Load the data from the file data = yaml.safe_load(file) # Get the API key (Groq example here) groq_api_key = data['GROQ_API_KEY'] Now, you can easily replace the \u201capi_key\u201d value directly with the loaded values without loading your environment variables.", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "3584fcd7c1"}, {"question": "Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?", "answer": "Option 1: using direnv created the .envrc file & added my API key, ran direnv allow in the terminal was getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\" resolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv. from dotenv import load_dotenv load_dotenv('.envrc') Option 2: using Codespaces Secrets Log in to your GitHub account and navigate to Settings > Codespaces There is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available. Once you set this up, the key will be available in your codespaces session", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "7c7c54c316"}, {"question": "OpenSource: How can I use Ollama open-source models locally on my pc without using any API?", "answer": "Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com To download ollama for Ubuntu: ``` curl -fsSL https://", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "2bc855c626"}, {"question": "OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?", "answer": "The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to get the number of tokens. You can use the code provided in the question to get the number of tokens.", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "46432879eb"}, {"question": "OpenSource: Can I use Groq instead of OpenAI?", "answer": "You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don\u2019t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "8810622759"}, {"question": "OpenSource: Can I use open-source alternatives to OpenAI API?", "answer": "Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.", "course": "llm-zoomcamp", "section": "Module 1: Introduction", "id": "9a97f6baa9"}, {"question": "SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?", "answer": "Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\n\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\n1- Navigate to Your Project Directory: \ncd /home/jovyan/my_project\n2- Configure GitHub Remote to Use SSH: \ngit remote set-url origin git@github.com:username/repository.git\n3- Stage, Commit and push your changes:\ngit add .\ngit commit -m \"Your commit message\"\ngit push", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "f7bddff24c"}, {"question": "SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?", "answer": "Clean out your cache using the following code:\nfrom transformers import TRANSFORMERS_CACHE\nprint(TRANSFORMERS_CACHE)\nimport shutil\nshutil.rmtree(TRANSFORMERS_CACHE)\nNote: Make sure to shutdown the notebook and restart the kernel", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "be64ebfdf3"}, {"question": "ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?", "answer": "Yes, you can. Here the step to follow:\n\n- Open a bash session in the elasticsearch container\n```bash\ndocker exec -it elasticsearch bash\n```\n- Add path.repo configuration:\n```bash\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\n```\n- Restart container and verify it was created correctly:\n```bash\ndocker restart elasticsearch\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\n```\n- Create the snapshot (this is the backup ;))\n```bash\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"your_index_name\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\n'\n```\n- Copy the backup to my machine:\n```bash\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\n```\n- Now create the new container or use docker-compose just in case you are following the module 2:\n```bash\ndocker compose up -d\n```\n- Add the path.repo configuration in the new one, same as before:\n```bash\ndocker exec -it new_elasticsearch bash\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\n```\n- Restart the docker container and copy the snapshot in it:\n```bash\ndocker restart new_elasticsearch\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\n```\n- Register the Snapshot Repository in the New Container:\n```bash\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H 'Content-Type: application/json' -d'\n{\n  \"type\": \"fs\",\n  \"settings\": {\n\t\"location\": \"/usr/share/elasticsearch/backup\"\n  }\n}\n'\n```\n- Verify if it exists:\n```bash\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\n```\n- Restore the snapshot:\n```bash\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"your_index_name\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\n'\n```\n- Show your indexes:\n```bash\ncurl -X GET \"localhost:9200/_cat/indices?v\"\n```\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\n```bash\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"old_index\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false,\n  \"rename_pattern\": \"old_index\",\n  \"rename_replacement\": \"new_index\"\n}\n'\n```", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "0955667c8b"}, {"question": "ElasticSearch: How can I limit the memory used by the ElasticSearch container?", "answer": "You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\n\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\n\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n    container_name: elasticsearch\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n    ports:\n      - \"9200:9200\"\n      - \"9300:9300\"\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'  # Limits to 1 CPU\n        reservations:\n          cpus: '0.5'  # Reserves 0.5 CPUs", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "c8c1a86d3a"}, {"question": "Docker: How to inspect the content of a file inside a Docker container?", "answer": "You have several ways to inspect the content of a file when you are inside a Docker container. \nFirst, make sure you ran the docker container interactively using bash:\ndocker exec -it <container> bash\nThen, you are able to use bash commands. For this case, I propose two solutions:\nUse \u201ccat\u201d and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\napt-get install vim\nvim your_file\nThen, you can exit your file in vim by pressing ESC then typing \u201c:q\u201d and finally press ENTER", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "e55abe2fdf"}, {"question": "Docker: Error: Docker mounted volume adds ;C to end of windows path", "answer": "Use the following line instead in mounting the current volume to docker for Q4:\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "3dc78d57bf"}, {"question": "Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?", "answer": "In Docker Desktop, try to increase the resource.\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "b21079429d"}, {"question": "Docker: How can to clean docker cache?", "answer": "docker system prune -a", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "743eff0db4"}, {"question": "Ollama: \u201cError: pull model manifest: 503: no healthy upstream\u201d when pulling a model with Ollama", "answer": "A network connection failure usually causes this error and if you try to repeat the operation immediately it\u2019ll still fail. It\u2019s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "9af629b821"}, {"question": "Ollama: Error: NotFoundError: Error code: 404 - {'error': {'message': \"model XXX not found, try pulling it first\" \u2026", "answer": "To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "77b0258ba3"}, {"question": "Ollama: Running Ollama locally on Colab gives error after the llm() line", "answer": "APIConnectionError: Connection error.\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\nFound a solution in the Medium article and this link: \nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\nhttps://github.com/ollama/ollama/issues/703", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "2b62c31e6e"}, {"question": "Ollama: How can remove Ollama model?", "answer": "ollama list\nollama rm [model_name]", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "4eaf9ed6b0"}, {"question": "Ollama: Error code 500 InternalServerError", "answer": "InternalServerError: Error code: 500 - {'error': {'message': 'model requires more system memory (5.6 GiB) than is available (1.5 GiB)', 'type': 'api_error', 'param': None, 'code': None}}.\n\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\n\nversion: '3.8'\n\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n    container_name: elasticsearch\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n      - ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\n    ports:\n      - \"9200:9200\"\n      - \"9300:9300\"\n    deploy:\n      resources:\n        limits:\n          memory: 2G  # change 2\n\n  ollama:\n    image: ollama/ollama\n    container_name: ollama\n    volumes:\n      - ollama:/root/.ollama\n    ports:\n      - \"11434:11434\"\n\nvolumes:\n  ollama:", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "72cccef6ad"}, {"question": "Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF", "answer": "Manually set the token as below:\naccess_token = <your_token>\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "ce9a2f1962"}, {"question": "Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'", "answer": "To solve just install transformers directly from github\n!pip install git+https://github.com/huggingface/transformers", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "811129de36"}, {"question": "Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3", "answer": "To solve just install transformers directly from github\n!pip install git+https://github.com/huggingface/transformers", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "fabe4ec378"}, {"question": "Python: from google.protobuf.pyext import _message / TypeError: bases must be types", "answer": "pip install protobuf==3.20.1", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "2a7ce62d02"}, {"question": "HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?", "answer": "1. search with the model name on hugging face.\n2. get the transformer used on the model.\n3. using the transformer, encode the string you want.\n4. calculate the length of the outputted tensor.\n\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM.\nDon\u2019t forget to make your token secret.", "course": "llm-zoomcamp", "section": "Module 2: Open-Source LLMs", "id": "2952b28d5e"}]